# Gradient Boosting and Ensemble Models: A Comprehensive Implementation

## Overview
This repository contains seven key tasks demonstrating advanced **Gradient Boosting** and **Ensemble Learning** techniques. The project includes:
- Decision trees from scratch
- Ensemble methods like Random Forest and AdaBoost
- Advanced Gradient Boosting frameworks (XGBoost, LightGBM, and CatBoost)
- Regression, Classification, and Ranking use cases

Each task is implemented in Python with proper evaluation and visualizations, showcasing best practices in data science.

Youtube: https://youtu.be/pvucm0STJ2Q 
---

## Tasks

### 1. **Classic Gradient Boosting Method (GBM) from Scratch**
Implemented GBM from scratch to demonstrate the mathematical and algorithmic foundation of gradient boosting. 
- Includes residual-based tree construction.
- Visualizes how predictions and residuals evolve over iterations.

**Key File**: `https://github.com/intimanjunath/Decision-trees-and-ensemble/blob/main/Classic_GBM_from_Scratch.ipynb`

---

### 2. **Random Forest from Scratch**
Built a random forest implementation from scratch using bootstrapped sampling and decision trees.
- Explains bagging and majority voting for classification.
- Demonstrates the scalability of ensemble methods over single decision trees.

**Key File**: `https://github.com/intimanjunath/Decision-trees-and-ensemble/blob/main/Random_Forest_Implementation_from_Scratch.ipynb`

---

### 3. **AdaBoost Algorithm from Scratch**
Constructed the AdaBoost algorithm step-by-step with weak learners (decision stumps).
- Shows how misclassified samples are iteratively reweighted.
- Highlights boostingâ€™s ability to improve weak classifiers.

**Key File**: `adaboost_from_scratch.ipynb](https://github.com/intimanjunath/Decision-trees-and-ensemble/blob/main/AdaBoost_Implementation.ipynb`

---

### 4. **Decision Trees from Scratch**
Implemented decision trees with both Gini impurity and entropy criteria.
- Includes recursive binary splits and leaf node predictions.
- Visualized the tree structure and feature importance.

**Key File**: `https://github.com/intimanjunath/Decision-trees-and-ensemble/blob/main/Decision_Tree_from_Scratch.ipynb`

---

### 5. **Showcasing GBM Classifier Techniques**
Explored multiple GBM-based classifiers:
- **XGBoost**, **LightGBM**, and **CatBoost** for classification.
- Benchmarked against Decision Tree, Random Forest, and AdaBoost.

**Key File**: `https://github.com/intimanjunath/Decision-trees-and-ensemble/blob/main/Showcase_gbm_classifier_techniques.ipynb`

---

### 6. **Showcasing Gradient Boost Regression Techniques**
Used GBM-based regression techniques:
- **XGBoostRegressor**, **LGBMRegressor**, and **CatBoostRegressor**.
- Evaluated on synthetic regression data, comparing RMSE across models.

**Key File**: `https://github.com/intimanjunath/Decision-trees-and-ensemble/blob/main/GradientBoosting_Regression.ipynb`

---

### 7. **Showcasing Gradient Boost Ranking Techniques**
Demonstrated ranking models using:
- **XGBRanker**, **LightGBM**, and **CatBoostRanker**.
- Generated synthetic ranking data to simulate search engine or recommendation systems.

**Key File**: `https://github.com/intimanjunath/Decision-trees-and-ensemble/blob/main/GradientBoosting_Ranking.ipynb`

---

## Installation
Clone the repository and install the required dependencies:
```bash
git clone https://github.com/your-username/ensemble-and-gbm-techniques.git
cd ensemble-and-gbm-techniques
pip install -r requirements.txt
